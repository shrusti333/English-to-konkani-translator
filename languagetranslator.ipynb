{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b54c48a-60e9-4305-b61f-a39bcf5b5453",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ea7633-17b8-48d1-8b7e-5380f6096dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513f1b40-c168-4388-9539-4c06fa30d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all variables \n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713ffb40-65f8-4b7a-8c30-09d0da87f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset file\n",
    "with open('try.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7503e4-456c-4870-bf52-2a7c7df22281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[:8]:\n",
    "    #split input and target by '\\t'=tab\n",
    "    input_text,target_text = row.split('\\t')\n",
    "    #add '\\t' at start and '\\n' at end of text.\n",
    "    target_text='\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text.lower())\n",
    "    target_texts.append(target_text)\n",
    "    #split character from text and add in respective sets\n",
    "    input_characters.update(list(input_text.lower()))\n",
    "    target_characters.update(list(target_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44dcf869-6435-4237-996a-2738797c2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort input and target characters \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3473c97-3cb7-4dfe-8daa-d518229b8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the total length of input and target characters\n",
    "num_en_chars = len(input_characters)\n",
    "num_dec_chars = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b1770e-0c34-41f5-a65b-8590248a0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the maximum length of input and target text.\n",
    "max_input_length = max([len(i) for i in input_texts])\n",
    "max_target_length = max([len(i) for i in target_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fad839-777b-44a8-a479-a2252f1a15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_texts,target_texts):\n",
    "  #inintialize encoder , decoder input and target data.\n",
    "  en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "  #padding variable with first character as 1 as rest all 0.\n",
    "  pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "  pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "  #countvectorizer for one hot encoding as we want to tokenize character so\n",
    "  #anlyzer is true and None the stopwords action.\n",
    "  cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')\n",
    "  for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "    #fit the input characters into the CountVectorizer function\n",
    "    cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "    #transform the input text from the help of CountVectorizer fit.\n",
    "    #it character present than put 1 and 0 otherwise.\n",
    "    en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "    cv_tar= cv.fit(target_characters)\t\t\n",
    "    dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "    #decoder target will be one timestep ahead because it will not consider \n",
    "    #the first character i.e. '\\t'.\n",
    "    dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "    \n",
    "    #add padding variable if the length of the input or target text is smaller\n",
    "    #than their respective maximum input or target length. \n",
    "    if len(input_t) < max_input_length:\n",
    "      for _ in range(max_input_length-len(input_t)):\n",
    "        en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)):\n",
    "        dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)+1):\n",
    "        dec_tr_data[i].append(pad_dec)\n",
    "  \n",
    "  #convert list to numpy array with data type float32\n",
    "  en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "  dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "  dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "\n",
    "  return en_in_data,dec_in_data,dec_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0fc78b1-dde5-4e4e-b511-9e78bcc863b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of encoder characters\n",
    "en_inputs = Input(shape=(None, num_en_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a377d0ba-af64-447b-ad5a-70dcc7981712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LSTM with the hidden dimension of 256\n",
    "#return state=True as we don't want output sequence.\n",
    "encoder = LSTM(256, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d26624-d713-474a-b4ff-5fc27a2ae623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard encoder output and store hidden and cell state.\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3aeb72-3280-41c1-a996-7a89bac07ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of decoder characters\n",
    "dec_inputs = Input(shape=(None, num_dec_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bb8b8f7-c66f-4fdb-bffb-428806af9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LSTM with the hidden dimension of 256\n",
    "#return state and return sequences as we want output sequence.\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d6012ed-6963-4dcb-a42c-e0126b25ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the decoder model with the states on encoder.\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c65a4339-198f-4392-90f2-019db8464dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer with shape of total number of decoder characters \n",
    "dec_dense = Dense(num_dec_chars, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1a5ed06-dfe3-4a4b-912c-3a824e57b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Model and store all variables \n",
    "model = Model([en_inputs, dec_inputs], dec_outputs)\n",
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters,\n",
    "             'max_input_length':max_input_length,'max_target_length':max_target_length,\n",
    "             'num_en_chars':num_en_chars,'num_dec_chars':num_dec_chars},open(\"training_data.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "858e178c-8c7e-4813-8040-cbc5f2aada6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:551: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 12s 12s/step - loss: 3.0491 - accuracy: 0.1296 - val_loss: 3.0251 - val_accuracy: 0.1667\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 3.0098 - accuracy: 0.3333 - val_loss: 3.0003 - val_accuracy: 0.4444\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.9687 - accuracy: 0.4630 - val_loss: 2.9717 - val_accuracy: 0.4444\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.9223 - accuracy: 0.4630 - val_loss: 2.9361 - val_accuracy: 0.4444\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.8656 - accuracy: 0.4444 - val_loss: 2.8889 - val_accuracy: 0.4444\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 2.7919 - accuracy: 0.4074 - val_loss: 2.8234 - val_accuracy: 0.4444\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 2.6915 - accuracy: 0.4074 - val_loss: 2.7291 - val_accuracy: 0.4444\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 2.5504 - accuracy: 0.4074 - val_loss: 2.5903 - val_accuracy: 0.4444\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 2.3545 - accuracy: 0.4074 - val_loss: 2.3871 - val_accuracy: 0.4444\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 2.1226 - accuracy: 0.4074 - val_loss: 2.1264 - val_accuracy: 0.4444\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 1.9850 - accuracy: 0.4074 - val_loss: 1.9520 - val_accuracy: 0.4444\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 2.0293 - accuracy: 0.4074 - val_loss: 1.9076 - val_accuracy: 0.4444\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 2.0517 - accuracy: 0.4074 - val_loss: 1.8970 - val_accuracy: 0.4444\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 1.9853 - accuracy: 0.4074 - val_loss: 1.8993 - val_accuracy: 0.4444\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.8760 - accuracy: 0.4074 - val_loss: 1.9226 - val_accuracy: 0.4444\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.7749 - accuracy: 0.4074 - val_loss: 1.9730 - val_accuracy: 0.4444\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.7174 - accuracy: 0.4259 - val_loss: 2.0424 - val_accuracy: 0.4444\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.7137 - accuracy: 0.4444 - val_loss: 2.1067 - val_accuracy: 0.4444\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.7378 - accuracy: 0.4444 - val_loss: 2.1427 - val_accuracy: 0.4444\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 1.7479 - accuracy: 0.4815 - val_loss: 2.1440 - val_accuracy: 0.4444\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.7258 - accuracy: 0.4815 - val_loss: 2.1159 - val_accuracy: 0.4444\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.6775 - accuracy: 0.4815 - val_loss: 2.0704 - val_accuracy: 0.4444\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.6211 - accuracy: 0.4815 - val_loss: 2.0248 - val_accuracy: 0.4444\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.5775 - accuracy: 0.4815 - val_loss: 1.9953 - val_accuracy: 0.4444\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.5580 - accuracy: 0.4815 - val_loss: 1.9875 - val_accuracy: 0.4444\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 1.5555 - accuracy: 0.4815 - val_loss: 1.9947 - val_accuracy: 0.4444\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 1.5522 - accuracy: 0.4815 - val_loss: 2.0074 - val_accuracy: 0.4444\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 1.5354 - accuracy: 0.5000 - val_loss: 2.0222 - val_accuracy: 0.4444\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.5070 - accuracy: 0.5185 - val_loss: 2.0418 - val_accuracy: 0.4444\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.4802 - accuracy: 0.4815 - val_loss: 2.0675 - val_accuracy: 0.3889\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.4649 - accuracy: 0.5370 - val_loss: 2.0931 - val_accuracy: 0.3889\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 1.4535 - accuracy: 0.4630 - val_loss: 2.1108 - val_accuracy: 0.3889\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.4305 - accuracy: 0.4630 - val_loss: 2.1220 - val_accuracy: 0.3889\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.3972 - accuracy: 0.4630 - val_loss: 2.1354 - val_accuracy: 0.3889\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.3686 - accuracy: 0.4259 - val_loss: 2.1571 - val_accuracy: 0.3889\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.3534 - accuracy: 0.4630 - val_loss: 2.1849 - val_accuracy: 0.3889\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 1.3437 - accuracy: 0.4630 - val_loss: 2.2125 - val_accuracy: 0.3889\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.3262 - accuracy: 0.4444 - val_loss: 2.2377 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.2981 - accuracy: 0.4444 - val_loss: 2.2643 - val_accuracy: 0.4444\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.2692 - accuracy: 0.5185 - val_loss: 2.2964 - val_accuracy: 0.3889\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.2485 - accuracy: 0.5556 - val_loss: 2.3314 - val_accuracy: 0.4444\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 1.2283 - accuracy: 0.5556 - val_loss: 2.3662 - val_accuracy: 0.4444\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.1958 - accuracy: 0.5370 - val_loss: 2.4082 - val_accuracy: 0.3889\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.1634 - accuracy: 0.5185 - val_loss: 2.4607 - val_accuracy: 0.3889\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.1416 - accuracy: 0.5000 - val_loss: 2.5129 - val_accuracy: 0.4444\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.1152 - accuracy: 0.5000 - val_loss: 2.5607 - val_accuracy: 0.4444\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 1.0819 - accuracy: 0.5556 - val_loss: 2.6115 - val_accuracy: 0.4444\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 1.0573 - accuracy: 0.6296 - val_loss: 2.6611 - val_accuracy: 0.4444\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.0277 - accuracy: 0.7037 - val_loss: 2.7065 - val_accuracy: 0.5000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.9952 - accuracy: 0.7037 - val_loss: 2.7523 - val_accuracy: 0.5556\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.9722 - accuracy: 0.6667 - val_loss: 2.7971 - val_accuracy: 0.6111\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.9406 - accuracy: 0.7037 - val_loss: 2.8481 - val_accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.9117 - accuracy: 0.7593 - val_loss: 2.9008 - val_accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.8815 - accuracy: 0.8148 - val_loss: 2.9546 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.8479 - accuracy: 0.7778 - val_loss: 3.0191 - val_accuracy: 0.5556\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.8206 - accuracy: 0.7778 - val_loss: 3.0890 - val_accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.7881 - accuracy: 0.7778 - val_loss: 3.1571 - val_accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.7606 - accuracy: 0.7963 - val_loss: 3.2085 - val_accuracy: 0.5556\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.7267 - accuracy: 0.8148 - val_loss: 3.2623 - val_accuracy: 0.6111\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.6983 - accuracy: 0.8333 - val_loss: 3.3256 - val_accuracy: 0.6111\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.6691 - accuracy: 0.8333 - val_loss: 3.3902 - val_accuracy: 0.6111\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.6372 - accuracy: 0.8519 - val_loss: 3.4624 - val_accuracy: 0.6111\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6077 - accuracy: 0.8519 - val_loss: 3.5462 - val_accuracy: 0.5556\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5753 - accuracy: 0.8704 - val_loss: 3.6241 - val_accuracy: 0.5556\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.5463 - accuracy: 0.8889 - val_loss: 3.6932 - val_accuracy: 0.5556\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5203 - accuracy: 0.9074 - val_loss: 3.7699 - val_accuracy: 0.5556\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.4926 - accuracy: 0.9444 - val_loss: 3.8323 - val_accuracy: 0.5556\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.4642 - accuracy: 0.9259 - val_loss: 3.9055 - val_accuracy: 0.5556\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.4385 - accuracy: 0.9444 - val_loss: 3.9829 - val_accuracy: 0.5556\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.4157 - accuracy: 0.9815 - val_loss: 4.0545 - val_accuracy: 0.6111\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.3958 - accuracy: 0.9444 - val_loss: 4.1439 - val_accuracy: 0.5000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3824 - accuracy: 0.9259 - val_loss: 4.2017 - val_accuracy: 0.6111\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.3740 - accuracy: 0.9259 - val_loss: 4.2790 - val_accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3420 - accuracy: 0.9630 - val_loss: 4.3291 - val_accuracy: 0.5556\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.3187 - accuracy: 0.9815 - val_loss: 4.3772 - val_accuracy: 0.6111\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3114 - accuracy: 0.9259 - val_loss: 4.4419 - val_accuracy: 0.5556\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2990 - accuracy: 0.9259 - val_loss: 4.4701 - val_accuracy: 0.6111\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2766 - accuracy: 0.9630 - val_loss: 4.5123 - val_accuracy: 0.6111\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.2628 - accuracy: 0.9630 - val_loss: 4.5688 - val_accuracy: 0.5556\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.2577 - accuracy: 0.9444 - val_loss: 4.5921 - val_accuracy: 0.6111\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.2460 - accuracy: 0.9444 - val_loss: 4.6401 - val_accuracy: 0.5556\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2290 - accuracy: 0.9815 - val_loss: 4.6739 - val_accuracy: 0.5556\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.2194 - accuracy: 0.9815 - val_loss: 4.6939 - val_accuracy: 0.6111\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.2159 - accuracy: 0.9444 - val_loss: 4.7367 - val_accuracy: 0.5556\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2089 - accuracy: 0.9630 - val_loss: 4.7453 - val_accuracy: 0.6111\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1952 - accuracy: 0.9630 - val_loss: 4.7724 - val_accuracy: 0.6111\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1865 - accuracy: 0.9630 - val_loss: 4.8082 - val_accuracy: 0.5556\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1839 - accuracy: 0.9815 - val_loss: 4.8224 - val_accuracy: 0.6111\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1789 - accuracy: 0.9630 - val_loss: 4.8568 - val_accuracy: 0.6111\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1697 - accuracy: 0.9815 - val_loss: 4.8732 - val_accuracy: 0.6111\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1616 - accuracy: 0.9815 - val_loss: 4.8901 - val_accuracy: 0.6111\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1583 - accuracy: 0.9630 - val_loss: 4.9207 - val_accuracy: 0.6111\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1566 - accuracy: 0.9630 - val_loss: 4.9245 - val_accuracy: 0.6111\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1510 - accuracy: 0.9630 - val_loss: 4.9486 - val_accuracy: 0.6111\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1437 - accuracy: 0.9815 - val_loss: 4.9611 - val_accuracy: 0.6111\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1378 - accuracy: 0.9815 - val_loss: 4.9766 - val_accuracy: 0.6111\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1347 - accuracy: 0.9815 - val_loss: 5.0016 - val_accuracy: 0.6111\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1334 - accuracy: 0.9815 - val_loss: 5.0086 - val_accuracy: 0.6111\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1314 - accuracy: 0.9630 - val_loss: 5.0365 - val_accuracy: 0.6111\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1282 - accuracy: 0.9815 - val_loss: 5.0381 - val_accuracy: 0.6111\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1227 - accuracy: 0.9630 - val_loss: 5.0575 - val_accuracy: 0.6111\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.1173 - accuracy: 0.9815 - val_loss: 5.0659 - val_accuracy: 0.6111\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1134 - accuracy: 0.9815 - val_loss: 5.0758 - val_accuracy: 0.6111\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1111 - accuracy: 0.9815 - val_loss: 5.0930 - val_accuracy: 0.6111\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1100 - accuracy: 0.9815 - val_loss: 5.0969 - val_accuracy: 0.6111\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.1093 - accuracy: 0.9630 - val_loss: 5.1198 - val_accuracy: 0.6111\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.1083 - accuracy: 0.9815 - val_loss: 5.1203 - val_accuracy: 0.6111\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1059 - accuracy: 0.9630 - val_loss: 5.1428 - val_accuracy: 0.6111\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1018 - accuracy: 0.9815 - val_loss: 5.1464 - val_accuracy: 0.6111\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0973 - accuracy: 0.9815 - val_loss: 5.1603 - val_accuracy: 0.6111\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0944 - accuracy: 0.9815 - val_loss: 5.1758 - val_accuracy: 0.6111\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0936 - accuracy: 0.9815 - val_loss: 5.1792 - val_accuracy: 0.6111\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0931 - accuracy: 0.9815 - val_loss: 5.1981 - val_accuracy: 0.6111\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0912 - accuracy: 0.9815 - val_loss: 5.1985 - val_accuracy: 0.6111\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0882 - accuracy: 0.9815 - val_loss: 5.2094 - val_accuracy: 0.6111\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0857 - accuracy: 0.9815 - val_loss: 5.2197 - val_accuracy: 0.6111\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0847 - accuracy: 0.9815 - val_loss: 5.2223 - val_accuracy: 0.6111\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0841 - accuracy: 0.9815 - val_loss: 5.2374 - val_accuracy: 0.6111\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0826 - accuracy: 0.9815 - val_loss: 5.2398 - val_accuracy: 0.6111\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0805 - accuracy: 0.9815 - val_loss: 5.2496 - val_accuracy: 0.6111\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0787 - accuracy: 0.9815 - val_loss: 5.2601 - val_accuracy: 0.6111\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0779 - accuracy: 0.9815 - val_loss: 5.2636 - val_accuracy: 0.6111\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0772 - accuracy: 0.9815 - val_loss: 5.2765 - val_accuracy: 0.6111\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0758 - accuracy: 0.9815 - val_loss: 5.2799 - val_accuracy: 0.6111\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0741 - accuracy: 0.9815 - val_loss: 5.2873 - val_accuracy: 0.6111\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0730 - accuracy: 0.9815 - val_loss: 5.2965 - val_accuracy: 0.6111\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0722 - accuracy: 0.9815 - val_loss: 5.2989 - val_accuracy: 0.6111\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0714 - accuracy: 0.9815 - val_loss: 5.3086 - val_accuracy: 0.6111\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0702 - accuracy: 0.9815 - val_loss: 5.3118 - val_accuracy: 0.6111\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0690 - accuracy: 0.9815 - val_loss: 5.3176 - val_accuracy: 0.6111\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0680 - accuracy: 0.9815 - val_loss: 5.3248 - val_accuracy: 0.6111\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0674 - accuracy: 0.9815 - val_loss: 5.3276 - val_accuracy: 0.6111\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0666 - accuracy: 0.9815 - val_loss: 5.3355 - val_accuracy: 0.6111\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0657 - accuracy: 0.9815 - val_loss: 5.3385 - val_accuracy: 0.6111\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0647 - accuracy: 0.9815 - val_loss: 5.3442 - val_accuracy: 0.6111\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0639 - accuracy: 0.9815 - val_loss: 5.3497 - val_accuracy: 0.6111\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0632 - accuracy: 0.9815 - val_loss: 5.3531 - val_accuracy: 0.6111\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0626 - accuracy: 0.9815 - val_loss: 5.3599 - val_accuracy: 0.6111\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0619 - accuracy: 0.9815 - val_loss: 5.3629 - val_accuracy: 0.6111\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0612 - accuracy: 0.9815 - val_loss: 5.3688 - val_accuracy: 0.6111\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0604 - accuracy: 0.9815 - val_loss: 5.3734 - val_accuracy: 0.6111\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0598 - accuracy: 0.9815 - val_loss: 5.3779 - val_accuracy: 0.6111\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0592 - accuracy: 0.9815 - val_loss: 5.3841 - val_accuracy: 0.6111\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0586 - accuracy: 0.9815 - val_loss: 5.3881 - val_accuracy: 0.6111\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 5.3942 - val_accuracy: 0.6111\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0574 - accuracy: 0.9815 - val_loss: 5.3990 - val_accuracy: 0.6111\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0568 - accuracy: 0.9815 - val_loss: 5.4039 - val_accuracy: 0.6111\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0563 - accuracy: 0.9815 - val_loss: 5.4098 - val_accuracy: 0.6111\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0558 - accuracy: 0.9815 - val_loss: 5.4138 - val_accuracy: 0.6111\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0553 - accuracy: 0.9815 - val_loss: 5.4193 - val_accuracy: 0.6111\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0548 - accuracy: 0.9815 - val_loss: 5.4237 - val_accuracy: 0.6111\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0543 - accuracy: 0.9815 - val_loss: 5.4279 - val_accuracy: 0.6111\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 5.4328 - val_accuracy: 0.6111\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0534 - accuracy: 0.9815 - val_loss: 5.4363 - val_accuracy: 0.6111\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0529 - accuracy: 0.9815 - val_loss: 5.4406 - val_accuracy: 0.6111\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0524 - accuracy: 0.9815 - val_loss: 5.4447 - val_accuracy: 0.6111\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0520 - accuracy: 0.9815 - val_loss: 5.4480 - val_accuracy: 0.6111\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0516 - accuracy: 0.9815 - val_loss: 5.4523 - val_accuracy: 0.6111\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0512 - accuracy: 0.9815 - val_loss: 5.4556 - val_accuracy: 0.6111\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 5.4591 - val_accuracy: 0.6111\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0504 - accuracy: 0.9815 - val_loss: 5.4630 - val_accuracy: 0.6111\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0500 - accuracy: 0.9815 - val_loss: 5.4661 - val_accuracy: 0.6111\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 5.4697 - val_accuracy: 0.6111\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0493 - accuracy: 0.9815 - val_loss: 5.4732 - val_accuracy: 0.6111\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0490 - accuracy: 0.9815 - val_loss: 5.4764 - val_accuracy: 0.6111\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0486 - accuracy: 0.9815 - val_loss: 5.4801 - val_accuracy: 0.6111\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0483 - accuracy: 0.9815 - val_loss: 5.4835 - val_accuracy: 0.6111\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0479 - accuracy: 0.9815 - val_loss: 5.4868 - val_accuracy: 0.6111\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0476 - accuracy: 0.9815 - val_loss: 5.4905 - val_accuracy: 0.6111\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0473 - accuracy: 0.9815 - val_loss: 5.4939 - val_accuracy: 0.6111\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0470 - accuracy: 0.9815 - val_loss: 5.4973 - val_accuracy: 0.6111\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0467 - accuracy: 0.9815 - val_loss: 5.5010 - val_accuracy: 0.6111\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0464 - accuracy: 0.9815 - val_loss: 5.5043 - val_accuracy: 0.6111\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0461 - accuracy: 0.9815 - val_loss: 5.5077 - val_accuracy: 0.6111\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0458 - accuracy: 0.9815 - val_loss: 5.5112 - val_accuracy: 0.6111\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0456 - accuracy: 0.9815 - val_loss: 5.5144 - val_accuracy: 0.6111\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0453 - accuracy: 0.9815 - val_loss: 5.5178 - val_accuracy: 0.6111\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0450 - accuracy: 0.9815 - val_loss: 5.5212 - val_accuracy: 0.6111\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0448 - accuracy: 0.9815 - val_loss: 5.5243 - val_accuracy: 0.6111\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0445 - accuracy: 0.9815 - val_loss: 5.5276 - val_accuracy: 0.6111\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0443 - accuracy: 0.9815 - val_loss: 5.5308 - val_accuracy: 0.6111\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0440 - accuracy: 0.9815 - val_loss: 5.5339 - val_accuracy: 0.6111\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0438 - accuracy: 0.9815 - val_loss: 5.5371 - val_accuracy: 0.6111\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0435 - accuracy: 0.9815 - val_loss: 5.5402 - val_accuracy: 0.6111\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0433 - accuracy: 0.9815 - val_loss: 5.5432 - val_accuracy: 0.6111\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0431 - accuracy: 0.9815 - val_loss: 5.5464 - val_accuracy: 0.6111\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0429 - accuracy: 0.9815 - val_loss: 5.5494 - val_accuracy: 0.6111\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0426 - accuracy: 0.9815 - val_loss: 5.5523 - val_accuracy: 0.6111\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0424 - accuracy: 0.9815 - val_loss: 5.5553 - val_accuracy: 0.6111\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0422 - accuracy: 0.9815 - val_loss: 5.5582 - val_accuracy: 0.6111\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0420 - accuracy: 0.9815 - val_loss: 5.5611 - val_accuracy: 0.6111\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0418 - accuracy: 0.9815 - val_loss: 5.5640 - val_accuracy: 0.6111\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0416 - accuracy: 0.9815 - val_loss: 5.5668 - val_accuracy: 0.6111\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0414 - accuracy: 0.9815 - val_loss: 5.5696 - val_accuracy: 0.6111\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0412 - accuracy: 0.9815 - val_loss: 5.5724 - val_accuracy: 0.6111\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0410 - accuracy: 0.9815 - val_loss: 5.5751 - val_accuracy: 0.6111\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0408 - accuracy: 0.9815 - val_loss: 5.5778 - val_accuracy: 0.6111\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0407 - accuracy: 0.9815 - val_loss: 5.5805 - val_accuracy: 0.6111\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0405 - accuracy: 0.9815 - val_loss: 5.5832 - val_accuracy: 0.6111\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0403 - accuracy: 0.9815 - val_loss: 5.5858 - val_accuracy: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 18)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 21)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 281600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  284672      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 21)     5397        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 571,669\n",
      "Trainable params: 571,669\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#load the data and train the model\n",
    "en_in_data,dec_in_data,dec_tr_data = bagofcharacters(input_texts,target_texts)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")\n",
    "\n",
    "#summary and model plot\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ad890b0-a80b-42a9-b658-8017a6e2c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d30e22cd-7554-4262-8f07-d59cdec30162",
   "metadata": {},
   "outputs": [],
   "source": [
    "BG_GRAY=\"#ABB2B9\"\n",
    "BG_COLOR=\"#000\"\n",
    "TEXT_COLOR=\"#FFF\"\n",
    "FONT=\"Melvetica 14\"\n",
    "FONT_BOLD=\"Melvetica 13 bold\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07d78b7f-eeb0-4280-9599-5276b48c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cad2dec-b694-49ed-b2e5-81cbc9f0dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangTRans:\n",
    "    def __init__(self):\n",
    "        #initialize tkinter window and load the file\n",
    "        self.window=Tk()\n",
    "        self.main_window()\n",
    "        self.datafile()\n",
    "\n",
    "    def datafile(self):\n",
    "        #get all datas from datafile and load the model.\n",
    "        datafile = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
    "        self.input_characters = datafile['input_characters']\n",
    "        self.target_characters = datafile['target_characters']\n",
    "        self.max_input_length = datafile['max_input_length']\n",
    "        self.max_target_length = datafile['max_target_length']\n",
    "        self.num_en_chars = datafile['num_en_chars']\n",
    "        self.num_dec_chars = datafile['num_dec_chars']\n",
    "        self.loadmodel()\n",
    "\n",
    "    #runwindow\n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "    \n",
    "    def main_window(self):\n",
    "        #add title to window and configure it\n",
    "        self.window.title(\"Language Translator\")\n",
    "        self.window.resizable(width=False,height=False)\n",
    "        self.window.configure(width=520,height=520,bg=BG_COLOR)\n",
    "    \n",
    "        head_label=Label(self.window,bg=BG_COLOR,fg=TEXT_COLOR,text=\"Welcome to DataFlair\",font=FONT_BOLD,pady=10)\n",
    "        head_label.place(relwidth=1)\n",
    "        line = Label(self.window,width=450,bg=BG_COLOR)\n",
    "        line.place(relwidth=1,rely=0.07,relheight=0.012)\n",
    "\n",
    "        #create text widget where input and output will be displayed\n",
    "        self.text_widget=Text(self.window,width=20,height=2,bg=\"#fff\",fg=\"#000\",font=FONT,padx=5,pady=5)\n",
    "        self.text_widget.place(relheight=0.745,relwidth=1,rely=0.08)\n",
    "        self.text_widget.configure(cursor=\"arrow\",state=DISABLED)\n",
    "\n",
    "        #create scrollbar\n",
    "        scrollbar=Scrollbar(self.text_widget)\n",
    "        scrollbar.place(relheight=1,relx=0.974)\n",
    "        scrollbar.configure(command=self.text_widget.yview)\n",
    "\n",
    "        #create bottom label where text widget will placed\n",
    "        bottom_label=Label(self.window,bg=BG_GRAY,height=80)\n",
    "        bottom_label.place(relwidth=1,rely=0.825)\n",
    "        #this is for user to put english text\n",
    "        self.msg_entry=Entry(bottom_label,bg=\"#2C3E50\",fg=TEXT_COLOR,font=FONT)\n",
    "        self.msg_entry.place(relwidth=0.788,relheight=0.06,rely=0.008,relx=0.008)\n",
    "        self.msg_entry.focus()\n",
    "        self.msg_entry.bind(\"<Return>\",self.on_enter)\n",
    "        #send button which will call on_enter function to send the text\n",
    "        send_button=Button(bottom_label,text=\"Send\",font=FONT_BOLD,width=8,bg=\"#fff\",command=lambda: self.on_enter(None))        \n",
    "        send_button.place(relx=0.80,rely=0.008,relheight=0.06,relwidth=0.20)\n",
    "\n",
    "    def loadmodel(self):\n",
    "        #Inference model\n",
    "        #load the model\n",
    "        model = models.load_model(\"s2s\")\n",
    "        #construct encoder model from the output of second layer\n",
    "        #discard the encoder output and store only states.\n",
    "        enc_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "        #add input object and state from the layer.\n",
    "        self.en_model = Model(model.input[0], [state_h_enc, state_c_enc])\n",
    "\n",
    "        #create Input object for hidden and cell state for decoder\n",
    "        #shape of layer with hidden or latent dimension\n",
    "        dec_state_input_h = Input(shape=(256,), name=\"input_3\")\n",
    "        dec_state_input_c = Input(shape=(256,), name=\"input_4\")\n",
    "        dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "\n",
    "        #add input from the encoder output and initialize with \n",
    "        #states.\n",
    "        dec_lstm = model.layers[3]\n",
    "        dec_outputs, state_h_dec, state_c_dec = dec_lstm(\n",
    "            model.input[1], initial_state=dec_states_inputs\n",
    "        )\n",
    "        dec_states = [state_h_dec, state_c_dec]\n",
    "        dec_dense = model.layers[4]\n",
    "        dec_outputs = dec_dense(dec_outputs)\n",
    "        #create Model with the input of decoder state input and encoder input\n",
    "        #and decoder output with the decoder states.\n",
    "        self.dec_model = Model(\n",
    "            [model.input[1]] + dec_states_inputs, [dec_outputs] + dec_states\n",
    "        )\n",
    "        \n",
    "    def decode_sequence(self,input_seq):\n",
    "        #create dict object to get character from the index.\n",
    "        reverse_target_char_index = dict(enumerate(self.target_characters))\n",
    "        #get the states from the user input sequence\n",
    "        states_value = self.en_model.predict(input_seq)\n",
    "\n",
    "        #fit target characters and \n",
    "        #initialize every first character to be 1 which is '\\t'.\n",
    "        #Generate empty target sequence of length 1.\n",
    "        co=cv.fit(self.target_characters) \n",
    "        target_seq=np.array([co.transform(list(\"\\t\")).toarray().tolist()],dtype=\"float32\")\n",
    "\n",
    "        #if the iteration reaches the end of text than it will be stop the it\n",
    "        stop_condition = False\n",
    "        #append every predicted character in decoded sentence\n",
    "        decoded_sentence = \"\"\n",
    "        while not stop_condition:\n",
    "            #get predicted output and discard hidden and cell state.\n",
    "            output_chars, h, c = self.dec_model.predict([target_seq] + states_value)\n",
    "\n",
    "            #get the index and from dictionary get character from it.\n",
    "            char_index = np.argmax(output_chars[0, -1, :])\n",
    "            text_char = reverse_target_char_index[char_index]\n",
    "            decoded_sentence += text_char\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if text_char == \"\\n\" or len(decoded_sentence) > self.max_target_length:\n",
    "                stop_condition = True\n",
    "            #update target sequence to the current character index.\n",
    "            target_seq = np.zeros((1, 1, self.num_dec_chars))\n",
    "            target_seq[0, 0, char_index] = 1.0\n",
    "            states_value = [h, c]\n",
    "        #return the decoded sentence\n",
    "        return decoded_sentence\n",
    "\n",
    "    def on_enter(self,event):\n",
    "        #get user query and bot response\n",
    "        msg=self.msg_entry.get()\n",
    "        self.my_msg(msg,\"English\")\n",
    "        self.deocded_output(msg,\"Decoded\")\n",
    "\n",
    "    def bagofcharacters(self,input_t):\n",
    "        cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char') \n",
    "        en_in_data=[] ; pad_en=[1]+[0]*(len(self.input_characters)-1)\n",
    "\n",
    "        cv_inp= cv.fit(self.input_characters)\n",
    "        en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "\n",
    "        if len(input_t)< self.max_input_length:\n",
    "          for _ in range(self.max_input_length-len(input_t)):\n",
    "            en_in_data[0].append(pad_en)\n",
    "    \n",
    "        return np.array(en_in_data,dtype=\"float32\")\n",
    "    \n",
    "    def deocded_output(self,msg,sender):\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        en_in_data = self.bagofcharacters(msg.lower()+\".\")\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+self.decode_sequence(en_in_data)\n",
    "                                +\"\\n\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        self.text_widget.see(END)\n",
    "    \n",
    "    def my_msg(self,msg,sender):\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0,END)\n",
    "        self.text_widget.configure(state=NORMAL)\n",
    "        self.text_widget.insert(END,str(sender)+\" : \"+str(msg)+\"\\n\")\n",
    "        self.text_widget.configure(state=DISABLED)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "769885e0-362f-4c37-bc5c-5052ac267994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:551: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run the file\n",
    "if __name__==\"__main__\":\n",
    "    LT = LangTRans()\n",
    "    LT.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eea005-7a35-4085-870a-dbbaa2335d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
