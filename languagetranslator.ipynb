{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b54c48a-60e9-4305-b61f-a39bcf5b5453",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ea7633-17b8-48d1-8b7e-5380f6096dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models,preprocessing\n",
    "from tensorflow.keras.utils import plot_model,to_categorical\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,Embedding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513f1b40-c168-4388-9539-4c06fa30d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all variables \n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713ffb40-65f8-4b7a-8c30-09d0da87f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset file\n",
    "with open('try.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.read().split('\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7503e4-456c-4870-bf52-2a7c7df22281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[0:4]:\n",
    "    #split input and target by '\\t'=tab\n",
    "    input_text,target_text = row.split('\\t')\n",
    "    #add '\\t' at start and '\\n' at end of text.\n",
    "    target_text='\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text.lower())\n",
    "    target_texts.append(target_text.lower())\n",
    "    #split character from text and add in respective sets\n",
    "    input_characters.update(list(input_text.lower()))\n",
    "    target_characters.update(list(target_text.lower()))\n",
    "   \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c446a1a-2595-42ce-9eba-ee9cd533e089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go.', \"i'm 19.\", \"i'm ok.\", 'i love you.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb66a4b-88e3-44ba-8dd1-569a8ee8cf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\t',\n",
       " '\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '.',\n",
       " '1',\n",
       " '9',\n",
       " 'ं',\n",
       " 'आ',\n",
       " 'क',\n",
       " 'ग',\n",
       " 'च',\n",
       " 'ज',\n",
       " 'त',\n",
       " 'ब',\n",
       " 'म',\n",
       " 'र',\n",
       " 'व',\n",
       " 'स',\n",
       " 'ह',\n",
       " 'ा',\n",
       " 'ु',\n",
       " 'े',\n",
       " 'ो'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db3a2b4-33e0-47e1-96dc-d4c80968a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort input and target characters \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3473c97-3cb7-4dfe-8daa-d518229b8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the total length of input and target characters\n",
    "num_en_chars = len(input_characters)\n",
    "num_dec_chars = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b1770e-0c34-41f5-a65b-8590248a0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the maximum length of input and target text.\n",
    "max_input_length = max([len(i) for i in input_texts])\n",
    "max_target_length = max([len(i) for i in target_texts])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85a0ba-1150-4f2f-a228-5e6a0c4e38b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fad839-777b-44a8-a479-a2252f1a15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_texts,target_texts):\n",
    "  #inintialize encoder , decoder input and target data.\n",
    "  en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "  #padding variable with first character as 1 as rest all 0.\n",
    "  pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "  pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "  #countvectorizer for one hot encoding as we want to tokenize character so\n",
    "  #anlyzer is true and None the stopwords action.\n",
    "  cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char')\n",
    "  for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "    #fit the input characters into the CountVectorizer function\n",
    "    cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "    #transform the input text from the help of CountVectorizer fit.\n",
    "    #it character present than put 1 and 0 otherwise.\n",
    "    en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "    cv_tar= cv.fit(target_characters)\t\t\n",
    "    dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "    #decoder target will be one timestep ahead because it will not consider \n",
    "    #the first character i.e. '\\t'.\n",
    "    dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "    \n",
    "    #add padding variable if the length of the input or target text is smaller\n",
    "    #than their respective maximum input or target length. \n",
    "    if len(input_t) < max_input_length:\n",
    "      for _ in range(max_input_length-len(input_t)):\n",
    "        en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)):\n",
    "        dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)+1):\n",
    "        dec_tr_data[i].append(pad_dec)\n",
    "  \n",
    "  #convert list to numpy array with data type float32\n",
    "  en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "  dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "  dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "  en_in_data\n",
    "\n",
    "  return en_in_data,dec_in_data,dec_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fc78b1-dde5-4e4e-b511-9e78bcc863b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of encoder characters\n",
    "en_inputs = Input(shape=(None, num_en_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a377d0ba-af64-447b-ad5a-70dcc7981712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LSTM with the hidden dimension of 256\n",
    "#return state=True as we don't want output sequence.\n",
    "encoder = LSTM(256, return_state=True,return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d26624-d713-474a-b4ff-5fc27a2ae623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discard encoder output and store hidden and cell state.\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3aeb72-3280-41c1-a996-7a89bac07ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of decoder characters\n",
    "dec_inputs = Input(shape=(None, num_dec_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f36c18-8368-4a4d-ac22-0de006876333",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding( num_dec_chars, 256 , mask_zero=True) (dec_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bb8b8f7-c66f-4fdb-bffb-428806af9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LSTM with the hidden dimension of 256\n",
    "#return state and return sequences as we want output sequence.\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6012ed-6963-4dcb-a42c-e0126b25ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the decoder model with the states on encoder.\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65a4339-198f-4392-90f2-019db8464dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer with shape of total number of decoder characters \n",
    "dec_dense = Dense(num_dec_chars, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a5ed06-dfe3-4a4b-912c-3a824e57b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Model and store all variables \n",
    "model = Model([en_inputs, dec_inputs], dec_outputs)\n",
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters,\n",
    "             'max_input_length':max_input_length,'max_target_length':max_target_length,\n",
    "             'num_en_chars':num_en_chars,'num_dec_chars':num_dec_chars},open(\"training_data.pkl\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f9279d-9714-44bc-879f-6e9bc2192a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_en_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "858e178c-8c7e-4813-8040-cbc5f2aada6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:551: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 9s 9s/step - loss: 3.1833 - accuracy: 0.0326\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 3.1087 - accuracy: 0.4674\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 3.0290 - accuracy: 0.5435\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.9349 - accuracy: 0.5435\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 2.8096 - accuracy: 0.5326\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 2.6156 - accuracy: 0.5326\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 2.2919 - accuracy: 0.5326\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.9552 - accuracy: 0.5326\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 2.1659 - accuracy: 0.5326\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 2.2032 - accuracy: 0.5326\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 2.0555 - accuracy: 0.5326\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 1.8906 - accuracy: 0.5326\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1.8555 - accuracy: 0.5326\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.9113 - accuracy: 0.5326\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.9382 - accuracy: 0.5326\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.9105 - accuracy: 0.5326\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.8482 - accuracy: 0.5326\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.7865 - accuracy: 0.5326\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 1.7583 - accuracy: 0.5326\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 1.7669 - accuracy: 0.5326\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.7821 - accuracy: 0.5326\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.7751 - accuracy: 0.5217\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.7442 - accuracy: 0.5217\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.7107 - accuracy: 0.5217\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.6952 - accuracy: 0.5435\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.6961 - accuracy: 0.5435\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 1.7005 - accuracy: 0.5543\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.6979 - accuracy: 0.5543\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1.6841 - accuracy: 0.5543\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.6613 - accuracy: 0.5543\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 1.6363 - accuracy: 0.5543\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 1.6173 - accuracy: 0.5543\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 1.6088 - accuracy: 0.5435\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 1.6037 - accuracy: 0.5326\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 1.5880 - accuracy: 0.5326\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.5661 - accuracy: 0.5543\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.5555 - accuracy: 0.5652\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1.5523 - accuracy: 0.5652\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.5367 - accuracy: 0.5543\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.5093 - accuracy: 0.5543\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.4937 - accuracy: 0.5543\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.4907 - accuracy: 0.5652\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1.4684 - accuracy: 0.5652\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 1.4497 - accuracy: 0.5761\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.4454 - accuracy: 0.5870\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 1.4201 - accuracy: 0.5978\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 1.4045 - accuracy: 0.5978\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.3883 - accuracy: 0.5761\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.3687 - accuracy: 0.5652\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1.3434 - accuracy: 0.5761\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1.3300 - accuracy: 0.5978\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 1.2929 - accuracy: 0.5761\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.2775 - accuracy: 0.5652\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.2647 - accuracy: 0.5870\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 1.2422 - accuracy: 0.5652\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 1.1959 - accuracy: 0.5870\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 1.1769 - accuracy: 0.5870\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 1.1973 - accuracy: 0.6413\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 1.1653 - accuracy: 0.6196\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 1.1370 - accuracy: 0.6413\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 1.0861 - accuracy: 0.6739\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 1.1132 - accuracy: 0.6522\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.0694 - accuracy: 0.7283\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 1.0341 - accuracy: 0.7174\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.0370 - accuracy: 0.6630\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.9735 - accuracy: 0.6522\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.9821 - accuracy: 0.6957\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.9276 - accuracy: 0.6630\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.9303 - accuracy: 0.6739\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.8934 - accuracy: 0.7609\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.8773 - accuracy: 0.7717\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.8560 - accuracy: 0.7174\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.8250 - accuracy: 0.7391\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.8158 - accuracy: 0.8043\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.7831 - accuracy: 0.8152\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.7755 - accuracy: 0.7717\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.7479 - accuracy: 0.8370\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.7262 - accuracy: 0.8261\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.7127 - accuracy: 0.7826\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.6833 - accuracy: 0.8696\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.6673 - accuracy: 0.8587\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.6547 - accuracy: 0.8043\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6321 - accuracy: 0.8696\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.6098 - accuracy: 0.8696\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.5979 - accuracy: 0.8587\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.5879 - accuracy: 0.9130\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5596 - accuracy: 0.9130\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5434 - accuracy: 0.9022\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.5398 - accuracy: 0.9130\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.5218 - accuracy: 0.9022\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5045 - accuracy: 0.9457\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.4840 - accuracy: 0.9239\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4671 - accuracy: 0.9348\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4563 - accuracy: 0.9348\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4501 - accuracy: 0.9348\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.4629 - accuracy: 0.9348\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4644 - accuracy: 0.8478\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4749 - accuracy: 0.9022\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.4070 - accuracy: 0.9783\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.4497 - accuracy: 0.8587\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.4036 - accuracy: 0.9674\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.3988 - accuracy: 0.9674\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.3897 - accuracy: 0.9239\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.3579 - accuracy: 0.9891\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.3693 - accuracy: 0.9891\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.3392 - accuracy: 0.9891\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.3469 - accuracy: 0.9674\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.3218 - accuracy: 0.9891\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.3272 - accuracy: 0.9783\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3052 - accuracy: 0.9891\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3085 - accuracy: 0.9674\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2889 - accuracy: 1.0000\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2920 - accuracy: 0.9891\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2747 - accuracy: 1.0000\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2751 - accuracy: 0.9891\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2601 - accuracy: 1.0000\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2609 - accuracy: 1.0000\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2468 - accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2470 - accuracy: 1.0000\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2337 - accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.2338 - accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2225 - accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.2206 - accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.2122 - accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2077 - accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.2030 - accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1959 - accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1934 - accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1856 - accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1833 - accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1770 - accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1729 - accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.1695 - accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1635 - accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1609 - accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1562 - accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.1520 - accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1493 - accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1446 - accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1415 - accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.1384 - accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.1342 - accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.1316 - accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1284 - accuracy: 1.0000\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1248 - accuracy: 1.0000\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1223 - accuracy: 1.0000\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1193 - accuracy: 1.0000\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1162 - accuracy: 1.0000\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1138 - accuracy: 1.0000\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1111 - accuracy: 1.0000\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1082 - accuracy: 1.0000\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1060 - accuracy: 1.0000\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1036 - accuracy: 1.0000\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.1009 - accuracy: 1.0000\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0988 - accuracy: 1.0000\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0967 - accuracy: 1.0000\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0943 - accuracy: 1.0000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0922 - accuracy: 1.0000\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0903 - accuracy: 1.0000\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0882 - accuracy: 1.0000\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0862 - accuracy: 1.0000\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0844 - accuracy: 1.0000\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0826 - accuracy: 1.0000\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0807 - accuracy: 1.0000\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0790 - accuracy: 1.0000\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0774 - accuracy: 1.0000\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0756 - accuracy: 1.0000\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0741 - accuracy: 1.0000\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0710 - accuracy: 1.0000\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0681 - accuracy: 1.0000\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0667 - accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0653 - accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0628 - accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0615 - accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0603 - accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0591 - accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0579 - accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0568 - accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0557 - accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0536 - accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0517 - accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0507 - accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0489 - accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0431 - accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0416 - accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0116 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: s2s\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 15)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 24)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 256),  278528      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  287744      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 24)     6168        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 572,440\n",
      "Trainable params: 572,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#load the data and train the model\n",
    "en_in_data,dec_in_data,dec_tr_data = bagofcharacters(input_texts,target_texts)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data,\n",
    "    batch_size=64,\n",
    "    epochs=300,\n",
    "    validation_split=0.0,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s\")\n",
    "\n",
    "#summary and model plot\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7deaaf08-51f4-4bc0-8fb4-57cf40ba7401",
   "metadata": {},
   "outputs": [],
   "source": [
    " def decode_sequence(input_seq):\n",
    "        datafile = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
    "        input_characters = datafile['input_characters']\n",
    "        target_characters = datafile['target_characters']\n",
    "        max_input_length = datafile['max_input_length']\n",
    "        max_target_length = datafile['max_target_length']\n",
    "        num_en_chars = datafile['num_en_chars']\n",
    "        num_dec_chars = datafile['num_dec_chars']\n",
    "      \n",
    "        #Inference model\n",
    "        #load the model\n",
    "        model = models.load_model(\"s2s\")\n",
    "        #construct encoder model from the output of second layer\n",
    "        #discard the encoder output and store only states.\n",
    "        enc_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "        #add input object and state from the layer.\n",
    "        en_model = Model(model.input[0], [state_h_enc, state_c_enc])\n",
    "\n",
    "        #create Input object for hidden and cell state for decoder\n",
    "        #shape of layer with hidden or latent dimension\n",
    "        dec_state_input_h = Input(shape=(256,), name=\"input_3\")\n",
    "        dec_state_input_c = Input(shape=(256,), name=\"input_5\")\n",
    "        dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
    "\n",
    "        #add input from the encoder output and initialize with \n",
    "        #states.\n",
    "        dec_lstm = model.layers[3]\n",
    "        dec_outputs, state_h_dec, state_c_dec = dec_lstm(\n",
    "            model.input[1], initial_state=dec_states_inputs\n",
    "        )\n",
    "        dec_states = [state_h_dec, state_c_dec]\n",
    "        dec_dense = model.layers[4]\n",
    "        dec_outputs = dec_dense(dec_outputs)\n",
    "        #create Model with the input of decoder state input and encoder input\n",
    "        #and decoder output with the decoder states.\n",
    "        dec_model = Model(\n",
    "            [model.input[1]] + dec_states_inputs, [dec_outputs] + dec_states\n",
    "        )\n",
    "        #create dict object to get character from the index.\n",
    "        reverse_target_char_index = dict(enumerate(target_characters))\n",
    "        #get the states from the user input sequence\n",
    "        states_value = en_model.predict(input_seq)\n",
    "\n",
    "        #fit target characters and \n",
    "        #initialize every first character to be 1 which is '\\t'.\n",
    "        #Generate empty target sequence of length 1.\n",
    "        cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char') \n",
    "       \n",
    "        co=cv.fit(target_characters) \n",
    "        target_seq=np.array([co.transform(list(\"\\t\")).toarray().tolist()],dtype=\"float32\")\n",
    "        \n",
    "        #if the iteration reaches the end of text than it will be stop the it\n",
    "        stop_condition = False\n",
    "        #append every predicted character in decoded sentence\n",
    "        decoded_sentence = \"\"\n",
    "        while not stop_condition:\n",
    "            #get predicted output and discard hidden and cell state.\n",
    "            output_chars, h, c = dec_model.predict([target_seq] + states_value)\n",
    "            \n",
    "            #get the index and from dictionary get character from it.\n",
    "            char_index = np.argmax(output_chars[0, -1, :])\n",
    "            text_char = reverse_target_char_index[char_index]\n",
    "            decoded_sentence += text_char\n",
    "\n",
    "            # Exit condition: either hit max length\n",
    "            # or find stop character.\n",
    "            if text_char == \"\\n\" or len(decoded_sentence) > max_target_length:\n",
    "                stop_condition = True\n",
    "            #update target sequence to the current character index.\n",
    "            target_seq = np.zeros((1, 1, num_dec_chars))\n",
    "            target_seq[0, 0, char_index] = 1.0\n",
    "            states_value = [h, c]\n",
    "        #return the decoded sentence\n",
    "        return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dc37a10-fc59-47d6-b0c5-7c12c3b7be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_t):\n",
    "        datafile = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
    "        input_characters = datafile['input_characters']\n",
    "        target_characters = datafile['target_characters']\n",
    "        max_input_length = datafile['max_input_length']\n",
    "        max_target_length = datafile['max_target_length']\n",
    "        num_en_chars = datafile['num_en_chars']\n",
    "        num_dec_chars = datafile['num_dec_chars']\n",
    "        cv=CountVectorizer(binary=True,tokenizer=lambda txt: txt.split(),stop_words=None,analyzer='char') \n",
    "        en_in_data=[] ; pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "\n",
    "        cv_inp= cv.fit(input_characters)\n",
    "        en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "\n",
    "        if len(input_t)< max_input_length:\n",
    "          for _ in range(max_input_length-len(input_t)):\n",
    "            en_in_data[0].append(pad_en)\n",
    "    \n",
    "        return np.array(en_in_data,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed8b9d-8591-48a6-a363-b5369e6db0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f53df985-a9b8-4916-9735-27feb010ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entry():\n",
    "    x=  input( 'Enter eng sentence : ' ) \n",
    "    en_in_data = bagofcharacters(x.lower()+\".\")\n",
    "    x=decode_sequence(en_in_data)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e78663ea-02e6-4f50-b298-08da40460606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter eng sentence :  i love you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " हांव तुजेर मोग करता.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0eab9d-7f9c-4384-a122-49063315d4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6564fe-9a3c-4035-bbbc-f7b46a321f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
